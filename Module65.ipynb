{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db22c150-e2a3-44c4-bf68-7c0af9b69004",
   "metadata": {},
   "source": [
    "## Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a type of linear regression that includes a penalty term equivalent to the absolute value of the magnitude of the coefficients. The key idea behind Lasso Regression is to enforce sparsity by shrinking some coefficients to zero, thus performing feature selection. This distinguishes it from ordinary least squares regression, which does not impose such penalties, and from Ridge Regression, which uses an L2 penalty (squared magnitude of coefficients) instead of the L1 penalty used in Lasso.\n",
    "\n",
    "## Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "The main advantage of using Lasso Regression in feature selection is its ability to automatically select a subset of important features by shrinking the coefficients of less important features to exactly zero. This makes the model simpler and more interpretable, especially when dealing with high-dimensional data with many potential predictors.\n",
    "\n",
    "## Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "In a Lasso Regression model, the coefficients are interpreted similarly to those in standard linear regression, but with the added context that some coefficients may be exactly zero, indicating that the corresponding features are not contributing to the model. Non-zero coefficients represent the features that have been deemed important by the model, with their values indicating the strength and direction of their association with the response variable.\n",
    "\n",
    "## Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "The primary tuning parameter in Lasso Regression is the regularization parameter (lambda, often denoted as Î±). This parameter controls the strength of the L1 penalty:\n",
    "\n",
    "- *Higher lambda*: Increases the penalty, leading to more coefficients being shrunk to zero. This can result in a simpler model with fewer features but might underfit the data.\n",
    "- *Lower lambda*: Reduces the penalty, leading to more coefficients being retained. This can capture more complexity but might result in overfitting.\n",
    "\n",
    "Finding the right balance is crucial for optimal model performance.\n",
    "\n",
    "## Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "Yes, Lasso Regression can be adapted for non-linear regression problems by using basis expansion techniques such as polynomial features, splines, or interaction terms to transform the original features into a higher-dimensional space. Once transformed, Lasso can be applied to this new feature space, allowing it to capture non-linear relationships while still performing feature selection.\n",
    "\n",
    "## Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "The primary difference between Ridge Regression and Lasso Regression lies in the type of regularization applied:\n",
    "\n",
    "- *Ridge Regression*: Uses L2 regularization (squared magnitude of coefficients), which tends to shrink coefficients uniformly but rarely to exactly zero. This results in models that include all features but with reduced magnitude of coefficients.\n",
    "- *Lasso Regression*: Uses L1 regularization (absolute magnitude of coefficients), which can shrink some coefficients to exactly zero, effectively performing feature selection and creating sparser models.\n",
    "\n",
    "## Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "Yes, Lasso Regression can handle multicollinearity by selecting one of the correlated features and shrinking the others to zero. This is particularly useful when there are many correlated predictors, as it helps to simplify the model by retaining only one representative feature from a group of correlated features, thereby reducing redundancy.\n",
    "\n",
    "## Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "The optimal value of the regularization parameter (lambda) in Lasso Regression is typically chosen using cross-validation. The process involves:\n",
    "\n",
    "1. *Splitting the data* into training and validation sets.\n",
    "2. *Training the Lasso model* with different lambda values on the training set.\n",
    "3. *Evaluating the performance* of each model on the validation set.\n",
    "4. *Selecting the lambda* that results in the best performance metric (e.g., lowest mean squared error, highest R-squared) on the validation set.\n",
    "\n",
    "Techniques such as k-fold cross-validation can help ensure that the selected lambda generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc3bc51-b09c-49ea-9dcf-8a12afbd0eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
