{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d8b05e2-2b58-4b4c-983b-0e2da6e1981d",
   "metadata": {},
   "source": [
    "## Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n",
    "\n",
    "Web scraping is the automated process of extracting data from websites. It is used to gather and analyze information from web pages to gain insights, automate data collection, and integrate data into applications. Three areas where web scraping is used are:\n",
    "1. *E-commerce price monitoring*\n",
    "2. *Market research and competitive analysis*\n",
    "3. *Content aggregation and news scraping*\n",
    "\n",
    "## Q2. What are the different methods used for Web Scraping?\n",
    "\n",
    "The different methods used for web scraping include:\n",
    "1. *Manual Copy-Pasting*\n",
    "2. *HTML Parsing*\n",
    "3. *DOM Parsing*\n",
    "4. *XPath*\n",
    "5. *CSS Selectors*\n",
    "6. *API Access*\n",
    "7. *Headless Browsers and Browser Automation (e.g., Selenium)*\n",
    "\n",
    "## Q3. What is Beautiful Soup? Why is it used?\n",
    "\n",
    "Beautiful Soup is a Python library used for parsing HTML and XML documents. It creates a parse tree for parsed pages that can be used to extract data easily. It is used because it simplifies the process of web scraping by providing Pythonic methods for navigating, searching, and modifying the parse tree.\n",
    "\n",
    "*Q4. Why is flask used in this Web Scraping project?*\n",
    "\n",
    "Flask is used in this web scraping project because it is a lightweight and flexible web framework for Python. It allows developers to quickly set up a web server to serve scraped data, create RESTful APIs, and manage routes and views for web applications.\n",
    "\n",
    "## Q5. Write the names of AWS services used in this project. Also, explain the use of each service.\n",
    "\n",
    "The AWS services used in this project and their uses are:\n",
    "1. *Amazon EC2 (Elastic Compute Cloud)*: Provides scalable virtual servers to run web scraping scripts and host web applications.\n",
    "2. *Amazon S3 (Simple Storage Service)*: Stores and retrieves large amounts of data, such as scraped data or logs.\n",
    "3. *AWS Lambda*: Executes code in response to triggers, enabling serverless computing to run scraping tasks without managing servers.\n",
    "4. *Amazon RDS (Relational Database Service)*: Manages relational databases to store structured data gathered from scraping.\n",
    "5. *Amazon CloudWatch*: Monitors and logs the performance and health of web scraping applications and AWS resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82a49a8-b799-4925-9c59-b43ba5c2d0d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
