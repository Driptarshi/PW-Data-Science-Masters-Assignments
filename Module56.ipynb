{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17732730-c978-4570-b09c-7019bd57ba99",
   "metadata": {},
   "source": [
    "## Q1: What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "\n",
    "*Min-Max Scaling*:\n",
    "Min-Max scaling is a normalization technique used to scale the features of a dataset to a fixed range, typically [0, 1] or [-1, 1]. This is done by subtracting the minimum value of each feature and then dividing by the range (max - min) of the feature.\n",
    "\n",
    "*Formula*:\n",
    "\\[ X' = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}} \\]\n",
    "\n",
    "For scaling to a range [a, b]:\n",
    "\\[ X' = a + \\frac{(X - X_{\\min})(b - a)}{X_{\\max} - X_{\\min}} \\]\n",
    "\n",
    "*Example*:\n",
    "Suppose we have a feature with values [2, 4, 6, 8, 10] and we want to scale it to the range [0, 1].\n",
    "\n",
    "1. Minimum value \\(X_{\\min} = 2\\)\n",
    "2. Maximum value \\(X_{\\max} = 10\\)\n",
    "\n",
    "Scaled values:\n",
    "\\[ X' = \\frac{X - 2}{10 - 2} = \\frac{X - 2}{8} \\]\n",
    "\n",
    "So,\n",
    "- 2 -> 0\n",
    "- 4 -> 0.25\n",
    "- 6 -> 0.5\n",
    "- 8 -> 0.75\n",
    "- 10 -> 1\n",
    "\n",
    "## Q2: What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "\n",
    "*Unit Vector Technique*:\n",
    "The Unit Vector technique (or normalization to unit norm) scales the feature vector such that its Euclidean length (L2 norm) is 1. Each data point is scaled individually to have unit norm.\n",
    "\n",
    "*Formula*:\n",
    "\\[ X' = \\frac{X}{\\|X\\|_2} \\]\n",
    "where \\(\\|X\\|_2\\) is the L2 norm of the vector \\(X\\).\n",
    "\n",
    "*Difference from Min-Max Scaling*:\n",
    "- Min-Max scaling normalizes data to a specific range.\n",
    "- Unit Vector scaling normalizes data to have a unit length.\n",
    "\n",
    "*Example*:\n",
    "Consider a feature vector \\(X = [1, 2, 2]\\).\n",
    "\n",
    "1. Calculate the L2 norm:\n",
    "\\[ \\|X\\|_2 = \\sqrt{1^2 + 2^2 + 2^2} = \\sqrt{9} = 3 \\]\n",
    "\n",
    "2. Normalize each component:\n",
    "\\[ X' = \\left[\\frac{1}{3}, \\frac{2}{3}, \\frac{2}{3}\\right] \\]\n",
    "\n",
    "## Q3: What is PCA (Principal Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "\n",
    "*Principal Component Analysis (PCA)*:\n",
    "PCA is a statistical technique used to emphasize variation and bring out strong patterns in a dataset. It transforms the data into a set of linearly uncorrelated variables called principal components. These components are ordered so that the first few retain most of the variation present in the original dataset.\n",
    "\n",
    "*Steps*:\n",
    "1. Standardize the data.\n",
    "2. Compute the covariance matrix.\n",
    "3. Calculate eigenvalues and eigenvectors.\n",
    "4. Sort eigenvalues and select the top k eigenvectors.\n",
    "5. Transform the original data.\n",
    "\n",
    "*Example*:\n",
    "Suppose we have a 2D dataset with points \\((x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\). PCA might find that most of the variance is along a line through the origin, and we can project our data onto this line (the first principal component) to reduce dimensionality while retaining most of the variance.\n",
    "\n",
    "## Q4: What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "*Relationship*:\n",
    "PCA can be used for feature extraction by transforming the original features into a new set of features (principal components) that are linear combinations of the original features. These new features capture the most variance in the data with the fewest components, thus simplifying the dataset.\n",
    "\n",
    "*Using PCA for Feature Extraction*:\n",
    "1. Compute the principal components.\n",
    "2. Select the top k principal components based on eigenvalues.\n",
    "3. Use these components as new features for your model.\n",
    "\n",
    "*Example*:\n",
    "Suppose we have a dataset with 100 features. PCA is applied and it turns out that 95% of the variance can be captured with 10 principal components. These 10 components can now be used as new features, reducing dimensionality from 100 to 10 while preserving most of the information.\n",
    "\n",
    "## Q5: You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "\n",
    "To preprocess the data using Min-Max scaling:\n",
    "1. Identify the range (minimum and maximum values) for each feature (price, rating, delivery time).\n",
    "2. Apply Min-Max scaling to each feature to transform them into a common scale, typically [0, 1] or [-1, 1].\n",
    "\n",
    "*Example*:\n",
    "- Suppose the price range is \\$5 to \\$50.\n",
    "- The rating range is 1 to 5.\n",
    "- The delivery time range is 10 to 60 minutes.\n",
    "\n",
    "For price:\n",
    "\\[ \\text{Scaled Price} = \\frac{\\text{Price} - 5}{50 - 5} \\]\n",
    "\n",
    "For rating:\n",
    "\\[ \\text{Scaled Rating} = \\frac{\\text{Rating} - 1}{5 - 1} \\]\n",
    "\n",
    "For delivery time:\n",
    "\\[ \\text{Scaled Delivery Time} = \\frac{\\text{Delivery Time} - 10}{60 - 10} \\]\n",
    "\n",
    "## Q6: You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "\n",
    "To reduce the dimensionality of the dataset using PCA:\n",
    "1. Standardize the dataset to have zero mean and unit variance.\n",
    "2. Compute the covariance matrix of the features.\n",
    "3. Calculate the eigenvalues and eigenvectors of the covariance matrix.\n",
    "4. Sort the eigenvalues in descending order and select the top k eigenvectors corresponding to the largest eigenvalues.\n",
    "5. Project the original data onto the selected eigenvectors to obtain the reduced feature set.\n",
    "\n",
    "*Example*:\n",
    "If the original dataset has 100 features, and after applying PCA, it is determined that 10 principal components capture 95% of the variance, you can reduce the dataset to these 10 components.\n",
    "\n",
    "## Q7: For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "\n",
    "To scale the values [1, 5, 10, 15, 20] to a range of -1 to 1:\n",
    "1. Minimum value \\(X_{\\min} = 1\\)\n",
    "2. Maximum value \\(X_{\\max} = 20\\)\n",
    "\n",
    "\\[ X' = -1 + \\frac{(X - 1)(1 - (-1))}{20 - 1} \\]\n",
    "\n",
    "\\[ X' = -1 + \\frac{(X - 1) \\cdot 2}{19} \\]\n",
    "\n",
    "So,\n",
    "- 1 -> -1\n",
    "- 5 -> \\(-1 + \\frac{(5 - 1) \\cdot 2}{19} = -1 + \\frac{8}{19} \\approx -0.58\\)\n",
    "- 10 -> \\(-1 + \\frac{(10 - 1) \\cdot 2}{19} = -1 + \\frac{18}{19} \\approx -0.05\\)\n",
    "- 15 -> \\(-1 + \\frac{(15 - 1) \\cdot 2}{19} = -1 + \\frac{28}{19} \\approx 0.47\\)\n",
    "- 20 -> 1\n",
    "\n",
    "## Q8: For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "To determine how many principal components to retain:\n",
    "1. Standardize the dataset.\n",
    "2. Apply PCA to compute the principal components.\n",
    "3. Analyze the explained variance ratio of each principal component.\n",
    "\n",
    "*Selection Criteria*:\n",
    "- Typically, choose enough components to explain a high percentage of variance, often 95% or more.\n",
    "\n",
    "*Example*:\n",
    "Suppose the explained variance ratios are:\n",
    "- PC1: 40%\n",
    "- PC2: 30%\n",
    "- PC3: 15%\n",
    "- PC4: 10%\n",
    "- PC5: 5%\n",
    "\n",
    "Cumulative explained variance for the first three components is 40% + 30% + 15% = 85%. If 85% is considered sufficient, you might choose to retain the first three components. However, if you require 95% variance explained, you might retain the first four components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfc9ab5-7d16-4bc6-a53b-065837a77030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
