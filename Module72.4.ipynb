{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592a4c00-13ab-4ed3-af81-98d6262ef724",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q1. Automated Feature Engineering and Model Pipeline\n",
    "\n",
    "#### Step-by-Step Solution\n",
    "\n",
    "# 1. Import necessary libraries and load the dataset:\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    # Load dataset (assuming a pandas DataFrame 'df' with target variable 'target')\n",
    "    df = pd.read_csv('your_dataset.csv')\n",
    "    X = df.drop(columns=['target'])\n",
    "    y = df['target']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "\n",
    "# 2. Automated Feature Selection:\n",
    "    feature_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    X_train_selected = feature_selector.fit_transform(X_train, y_train)\n",
    "    X_test_selected = feature_selector.transform(X_test)\n",
    "    \n",
    "\n",
    "# 3. Numerical Pipeline:\n",
    "\n",
    "    numerical_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "\n",
    "# 4. Categorical Pipeline:\n",
    "\n",
    "    categorical_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "\n",
    "# 5. Combine Pipelines using ColumnTransformer:\n",
    "\n",
    "    from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', numerical_pipeline, selector(dtype_exclude='object')),\n",
    "        ('cat', categorical_pipeline, selector(dtype_include='object'))\n",
    "    ])\n",
    "    \n",
    "\n",
    "# 6. Build the final pipeline with Random Forest Classifier:\n",
    "\n",
    "    model_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    ])\n",
    "\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "\n",
    "# 7. Evaluate the model:\n",
    "\n",
    "    y_pred = model_pipeline.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Model Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "\n",
    "#### Interpretation of the Results and Possible Improvements\n",
    "- The model accuracy indicates how well the Random Forest Classifier performs on the test set.\n",
    "- If the accuracy is satisfactory, the pipeline can be used for predictions. If not, consider the following improvements:\n",
    "  - *Hyperparameter tuning*: Use GridSearchCV or RandomizedSearchCV to find the best parameters for the Random Forest Classifier.\n",
    "  - *Feature Engineering*: Create new features based on domain knowledge.\n",
    "  - *Handling Correlated Features*: Use PCA or remove highly correlated features before model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72303091-f39d-410a-aa9b-bf89bce3d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q2. Voting Classifier Pipeline on Iris Dataset\n",
    "\n",
    "#### Step-by-Step Solution\n",
    "\n",
    "# 1. Import necessary libraries and load the Iris dataset:\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    # Load Iris dataset\n",
    "    iris = load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "\n",
    "# 2. Create Pipelines for Random Forest and Logistic Regression:\n",
    "    rf_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    ])\n",
    "\n",
    "    lr_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(random_state=42))\n",
    "    ])\n",
    "    \n",
    "\n",
    "# 3. Combine the pipelines using a Voting Classifier:\n",
    "    voting_classifier = VotingClassifier(estimators=[\n",
    "        ('rf', rf_pipeline),\n",
    "        ('lr', lr_pipeline)\n",
    "    ], voting='soft')\n",
    "\n",
    "    voting_classifier.fit(X_train, y_train)\n",
    "    \n",
    "\n",
    "# 4. Evaluate the Voting Classifier:\n",
    "\n",
    "    y_pred = voting_classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Voting Classifier Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "\n",
    "# Interpretation of the Results\n",
    "# - The accuracy of the voting classifier indicates how well the combined predictions of the Random Forest and Logistic Regression models perform.\n",
    "# - If the accuracy is satisfactory, the model can be used for predictions.\n",
    "# - If not, consider the following improvements:\n",
    "#  - Hyperparameter tuning: Optimize the parameters for each individual classifier.\n",
    "#  - Different classifiers: Experiment with other classifiers to see if they improve the ensemble's performance.\n",
    "#  - Feature Engineering: Enhance the dataset with additional relevant features or transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16566d25-d7a5-457b-9e06-6b6dd509e6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
